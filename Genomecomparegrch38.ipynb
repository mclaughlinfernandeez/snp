{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJOFXiHti0ROIGWwW4QS6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mclaughlinfernandeez/snp/blob/main/Genomecomparegrch38.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW9rVuZRkdR9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a99e9480"
      },
      "source": [
        "## Web Application Frontend: File Uploads\n",
        "\n",
        "### Subtask:\n",
        "Outline the design and functionality of the web application frontend specifically for handling user file uploads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "752f44ac"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the high-level structure of the `polymorphism-processor-service` and where the GRCh38 checking logic fits in. This involves explaining:\n",
        "- The entry point of the service (e.g., an HTTP endpoint or a Pub/Sub subscriber).\n",
        "- How the service receives the input polymorphism data (e.g., file path in GCS).\n",
        "- The sequence of operations: loading reference, parsing input, performing validation, handling results, generating output formats, and uploading to the output bucket.\n",
        "- How errors are handled and reported throughout the process.\n",
        "- The key functions or modules that would encapsulate the logic for each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3bc02b"
      },
      "source": [
        "## Integrate into the Processor Service\n",
        "\n",
        "### Subtask:\n",
        "Outline how this checking logic will be integrated into the overall `polymorphism-processor-service` code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38a00ee7"
      },
      "source": [
        "## Handle Validation Results\n",
        "\n",
        "### Subtask:\n",
        "Describe how the service will handle valid and invalid entries, potentially generating a report or filtering the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "883c8e7a"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the specific steps and logic for validating each polymorphism entry against the GRCh38 reference. This includes:\n",
        "- How to access the relevant part of the reference genome based on the chromosome and position from the polymorphism data.\n",
        "- How to compare the reference allele in the polymorphism data with the actual nucleotide(s) at the specified position in the GRCh38 reference.\n",
        "- How to handle different types of polymorphisms (SNPs, indels) if applicable.\n",
        "- How to account for potential discrepancies in chromosome naming (e.g., 'chr1' vs '1').\n",
        "- How to use the loaded GRCh38 reference data structure (e.g., `pyfaidx` object) for efficient lookups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41ec963e"
      },
      "source": [
        "# Assuming you have loaded the GRCh38 reference using pyfaidx as suggested in the loading step:\n",
        "# from pyfaidx import Fasta\n",
        "# grch38_ref = Fasta('/path/to/GRCh38.fasta') # Replace with the actual path or object from GCS streaming/download\n",
        "\n",
        "# Assuming you have parsed the polymorphism data into a list of dictionaries:\n",
        "# polymorphisms = [\n",
        "#     {'chromosome': 'chr1', 'position': 100, 'reference_allele': 'A', 'observed_allele': 'T'},\n",
        "#     {'chromosome':s'1', 'position': 150, 'reference_allele': 'G', 'observed_allele': 'C'},\n",
        "#     {'chromosome': 'chrM', 'position': 50, 'reference_allele': 'C', 'observed_allele': 'T'},\n",
        "#     # Add more polymorphism entries\n",
        "# ]\n",
        "\n",
        "import logging\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def log_error(message, polymorphism=None, e=None):\n",
        "    \"\"\"Logs an error with optional polymorphism details and exception.\"\"\"\n",
        "    details = \"\"\n",
        "    if polymorphism:\n",
        "        details += f\"Polymorphism: Chromosome={polymorphism.get('chromosome')}, Position={polymorphism.get('position')}, Ref={polymorphism.get('reference_allele')}, Obs={polymorphism.get('observed_allele')}. \"\n",
        "    if e:\n",
        "        details += f\"Error: {e}\"\n",
        "    logging.error(f\"{message} {details}\")\n",
        "\n",
        "def handle_validation_error(polymorphism, error_type, e=None):\n",
        "    \"\"\"Handles and logs specific validation errors.\"\"\"\n",
        "    status_message = \"\"\n",
        "    if error_type == \"Chromosome not found\":\n",
        "        status_message = f\"Chromosome '{polymorphism.get('chromosome')}' not found in reference\"\n",
        "        log_error(status_message, polymorphism)\n",
        "    elif error_type == \"Position out of bounds\":\n",
        "        status_message = f\"Position {polymorphism.get('position')} is out of bounds for chromosome {polymorphism.get('chromosome')}\"\n",
        "        log_error(status_message, polymorphism)\n",
        "    elif error_type == \"Invalid reference allele\":\n",
        "         expected = e.args[0] if e and e.args else \"N/A\"\n",
        "         found = polymorphism.get('reference_allele')\n",
        "         status_message = f\"Invalid reference allele: Expected '{expected}', found '{found}' at {polymorphism.get('chromosome')}:{polymorphism.get('position')}\"\n",
        "         log_error(status_message, polymorphism)\n",
        "    elif error_type == \"General validation error\":\n",
        "        status_message = f\"An error occurred during validation\"\n",
        "        log_error(status_message, polymorphism, e)\n",
        "    else:\n",
        "        status_message = \"Unknown validation error\"\n",
        "        log_error(status_message, polymorphism, e)\n",
        "\n",
        "    return False, status_message\n",
        "\n",
        "\n",
        "def validate_polymorphism(polymorphism, grch38_ref):\n",
        "    \"\"\"\n",
        "    Validates a single polymorphism entry against the GRCh38 reference.\n",
        "\n",
        "    Args:\n",
        "        polymorphism (dict): A dictionary representing a single polymorphism\n",
        "                             with keys 'chromosome', 'position', 'reference_allele',\n",
        "                             and 'observed_allele'.\n",
        "        grch38_ref (pyfaidx.Fasta): The loaded GRCh38 reference genome object.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the polymorphism's reference allele matches the GRCh38 reference,\n",
        "              False otherwise.\n",
        "        str: A status message (\"Valid\", \"Invalid reference allele\", \"Chromosome not found\", \"Position out of bounds\").\n",
        "    \"\"\"\n",
        "    chrom = polymorphism.get('chromosome')\n",
        "    pos = polymorphism.get('position')\n",
        "    ref_allele_polymorphism = polymorphism.get('reference_allele')\n",
        "\n",
        "    if not chrom or pos is None or not ref_allele_polymorphism:\n",
        "        return handle_validation_error(polymorphism, \"General validation error\", e=ValueError(\"Missing required fields\"))\n",
        "\n",
        "\n",
        "    # Handle potential chromosome naming discrepancies (e.g., 'chr1' vs '1')\n",
        "    # You might need a mapping or try both 'chrX' and 'X'\n",
        "    if chrom not in grch38_ref:\n",
        "        if f'chr{chrom}' in grch38_ref:\n",
        "            chrom = f'chr{chrom}'\n",
        "        elif chrom.startswith('chr') and chrom[3:] in grch38_ref:\n",
        "             chrom = chrom[3:]\n",
        "        else:\n",
        "            return handle_validation_error(polymorphism, \"Chromosome not found\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # pyfaidx uses 1-based indexing for fetching sequences\n",
        "        # The polymorphism position is likely 1-based, so no adjustment needed for pyfaidx fetch\n",
        "        # If your polymorphism positions are 0-based, you would need to add 1: pos + 1\n",
        "        # Fetch the reference allele(s) at the specified position\n",
        "        # For a SNP, fetch a single nucleotide. For an indel, the logic would be more complex\n",
        "        # and might involve fetching flanking sequences.\n",
        "        ref_allele_grch38 = grch38_ref[chrom][pos - 1].seq # pyfaidx uses 0-based indexing for slicing, so subtract 1\n",
        "\n",
        "        # Compare the reference alleles (case-insensitive comparison is often safer)\n",
        "        if ref_allele_polymorphism.upper() == ref_allele_grch38.upper():\n",
        "            return True, \"Valid\"\n",
        "        else:\n",
        "            return handle_validation_error(polymorphism, \"Invalid reference allele\", e=ValueError(ref_allele_grch38))\n",
        "\n",
        "    except IndexError:\n",
        "        return handle_validation_error(polymorphism, \"Position out of bounds\")\n",
        "    except Exception as e:\n",
        "        return handle_validation_error(polymorphism, \"General validation error\", e)\n",
        "\n",
        "def process_polymorphism_file(input_filepath, output_filepath, grch38_ref):\n",
        "    \"\"\"\n",
        "    Reads polymorphism data from a CSV file, validates each entry, and writes\n",
        "    the results to a new CSV file.\n",
        "\n",
        "    Args:\n",
        "        input_filepath (str): The path to the input CSV file containing the full SNP set.\n",
        "        output_filepath (str): The path where the output CSV file with validation results will be saved.\n",
        "        grch38_ref (pyfaidx.Fasta): The loaded GRCh38 reference genome object.\n",
        "                                   This object should be created by loading the full GRCh38 fasta file.\n",
        "    \"\"\"\n",
        "    valid_polymorphisms = []\n",
        "    invalid_polymorphisms = []\n",
        "\n",
        "    with open(input_filepath, 'r', newline='') as infile, \\\n",
        "         open(output_filepath, 'w', newline='') as outfile:\n",
        "\n",
        "        reader = csv.DictReader(infile)\n",
        "        fieldnames = reader.fieldnames + ['validation_status', 'is_valid']\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for row in reader:\n",
        "            # Assuming your CSV has columns named 'chromosome', 'position', 'reference_allele', 'observed_allele'\n",
        "            # Adjust these names if your CSV uses different headers\n",
        "            polymorphism = {\n",
        "                'chromosome': row.get('chromosome'),\n",
        "                'position': int(row.get('position')) if row.get('position') else None,\n",
        "                'reference_allele': row.get('reference_allele'),\n",
        "                'observed_allele': row.get('observed_allele')\n",
        "            }\n",
        "\n",
        "            is_valid, status = validate_polymorphism(polymorphism, grch38_ref)\n",
        "\n",
        "            row['validation_status'] = status\n",
        "            row['is_valid'] = is_valid\n",
        "            writer.writerow(row)\n",
        "\n",
        "            if is_valid:\n",
        "                valid_polymorphisms.append(row)\n",
        "            else:\n",
        "                invalid_polymorphisms.append(row)\n",
        "\n",
        "    logging.info(f\"Processing complete. Valid entries: {len(valid_polymorphisms)}, Invalid entries: {len(invalid_polymorphisms)}\")\n",
        "    logging.info(f\"Results written to {output_filepath}\")\n",
        "\n",
        "# Example usage (replace with actual file paths and loaded GRCh38 reference):\n",
        "# from pyfaidx import Fasta\n",
        "# grch38_ref_path = '/path/to/your/GRCh38.fasta' # <<< Specify the path to your full GRCh38 fasta file\n",
        "# grch38_ref = Fasta(grch38_ref_path)\n",
        "\n",
        "# input_csv_path = '/path/to/your/full_snp_set.csv' # <<< Specify the path to your input CSV file with the full SNP set\n",
        "# output_csv_path = '/path/to/your/validation_results.csv' # <<< Specify the desired path for the output results file\n",
        "\n",
        "# process_polymorphism_file(input_csv_path, output_csv_path, grch38_ref)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea71e523"
      },
      "source": [
        "u%%writefile Dockerfile.frontend\n",
        "# Stage 1: Build the frontend (if needed - e.g., for building static assets with Node.js)\n",
        "# Example using Node.js to build a React/Angular app:\n",
        "# FROM node:16 as builder\n",
        "# WORKDIR /app\n",
        "# COPY package*.json ./\n",
        "# RUN npm install\n",
        "# COPY . .\n",
        "# RUN npm run build # Command to build your static assets\n",
        "\n",
        "# Stage 2: Serve the static files with Nginx\n",
        "FROM nginx:latest\n",
        "\n",
        "# Copy the built frontend files from the builder stage (adjust path if using a builder stage)\n",
        "# If no builder stage is needed (just serving static files), copy directly from the source\n",
        "COPY ./frontend /usr/share/nginx/html\n",
        "\n",
        "# Expose port 80, the default port for Nginx\n",
        "EXPOSE 80\n",
        "\n",
        "# The default command of the Nginx image starts the server, so no explicit CMD is needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cb7f4ae"
      },
      "source": [
        "## Build and push container images\n",
        "\n",
        "### Subtask:\n",
        "Build the Docker images for both services and push them to Google Cloud Container Registry or Artifact Registry.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "049b8922"
      },
      "source": [
        "**Reasoning**:\n",
        "Authenticate Docker with Google Cloud and then build and tag the Docker images for both the processor service and the frontend application based on the instructions provided.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f342742"
      },
      "source": [
        "!gcloud auth configure-docker us-central1-docker.pkg.dev\n",
        "\n",
        "# Replace 'your-gcp-project-id' and 'your-repo' with your actual GCP project ID and Artifact Registry repository name\n",
        "PROJECT_ID = 'your-gcp-project-id'\n",
        "REPO_NAME = 'your-repo'\n",
        "PROCESSOR_IMAGE = f'us-central1-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/polymorphism-processor-service:latest'\n",
        "FRONTEND_IMAGE = f'us-central1-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/webapp-frontend:latest'\n",
        "\n",
        "# Build and tag the processor service image\n",
        "!docker build -t {PROCESSOR_IMAGE} -f Dockerfile .\n",
        "\n",
        "# Build and tag the frontend application image\n",
        "# Assuming your frontend files are in a directory named 'frontend' relative to the Dockerfile.frontend\n",
        "# If your frontend files are in the same directory as Dockerfile.frontend, remove the './frontend' part in the COPY instruction in Dockerfile.frontend\n",
        "!docker build -t {FRONTEND_IMAGE} -f Dockerfile.frontend .\n",
        "\n",
        "# Push both images to Artifact Registry\n",
        "!docker push {PROCESSOR_IMAGE}\n",
        "!docker push {FRONTEND_IMAGE}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d75b404e"
      },
      "source": [
        "# Task\n",
        "Refactor the docker build commands into a single cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad801f02"
      },
      "source": [
        "**Reasoning**:\n",
        "Configure the Google Cloud project by setting the project ID and enabling the necessary APIs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a62764ae"
      },
      "source": [
        "## Set up google cloud project and services\n",
        "\n",
        "### Subtask:\n",
        "Configure your Google Cloud project, including enabling necessary APIs (Cloud Run, Cloud Storage, Pub/Sub), and setting up service accounts with appropriate permissions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa7505cd"
      },
      "source": [
        "**Reasoning**:\n",
        "Authenticate gcloud and then set the project and enable the services as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rwkUKE2s4WAl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0badf7e2"
      },
      "source": [
        "## Containerize the processor service\n",
        "\n",
        "### Subtask:\n",
        "Create a Dockerfile for the `polymorphism-processor-service` to package it as a container image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff74aff4"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the Dockerfile for the polymorphism-processor-service with the specified steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "411776e4"
      },
      "source": [
        "%%writefile Dockerfile\n",
        "# Stage 1: Build the application\n",
        "FROM python:3.9 as builder\n",
        "\n",
        "# Set the working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy requirements file and install dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the rest of the application code\n",
        "COPY . .\n",
        "\n",
        "# Stage 2: Create the final lightweight image\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy only the installed packages from the builder stage\n",
        "COPY --from=builder /usr/local/lib/python3.9/site-packages /usr/local/lib/python3.9/site-packages\n",
        "\n",
        "# Copy the application code from the builder stage\n",
        "COPY --from=builder /app /app\n",
        "\n",
        "# Make port 8080 available to the world outside this container\n",
        "EXPOSE 8080\n",
        "\n",
        "# Define environment variable\n",
        "ENV NAME World\n",
        "\n",
        "# Run app.py when the container launches\n",
        "CMD [\"python\", \"main.py\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb129249"
      },
      "source": [
        "# Task\n",
        "Refactor the Dockerfile and Dockerfile.frontend to use multi-stage builds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9634370d"
      },
      "source": [
        "## Set up google cloud project and services\n",
        "\n",
        "### Subtask:\n",
        "Configure your Google Cloud project, including enabling necessary APIs (Cloud Run, Cloud Storage, Pub/Sub), and setting up service accounts with appropriate permissions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24559471"
      },
      "source": [
        "**Reasoning**:\n",
        "Authenticate gcloud and then set the project and enable the services as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8489a7cb"
      },
      "source": [
        "**Reasoning**:\n",
        "Authenticate gcloud with the provided authorization code and then set the project and enable the services as requested by the instructions.\n",
        "\n"
      ]
    }
  ]
}