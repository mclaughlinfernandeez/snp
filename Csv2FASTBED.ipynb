{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw5YxZZntJhUURmFn7zP7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mclaughlinfernandeez/snp/blob/main/Csv2FASTBED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW9rVuZRkdR9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a99e9480"
      },
      "source": [
        "## Web Application Frontend: File Uploads\n",
        "\n",
        "### Subtask:\n",
        "Outline the design and functionality of the web application frontend specifically for handling user file uploads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "752f44ac"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the high-level structure of the `polymorphism-processor-service` and where the GRCh38 checking logic fits in. This involves explaining:\n",
        "- The entry point of the service (e.g., an HTTP endpoint or a Pub/Sub subscriber).\n",
        "- How the service receives the input polymorphism data (e.g., file path in GCS).\n",
        "- The sequence of operations: loading reference, parsing input, performing validation, handling results, generating output formats, and uploading to the output bucket.\n",
        "- How errors are handled and reported throughout the process.\n",
        "- The key functions or modules that would encapsulate the logic for each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3bc02b"
      },
      "source": [
        "## Integrate into the Processor Service\n",
        "\n",
        "### Subtask:\n",
        "Outline how this checking logic will be integrated into the overall `polymorphism-processor-service` code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38a00ee7"
      },
      "source": [
        "## Handle Validation Results\n",
        "\n",
        "### Subtask:\n",
        "Describe how the service will handle valid and invalid entries, potentially generating a report or filtering the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "883c8e7a"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the specific steps and logic for validating each polymorphism entry against the GRCh38 reference. This includes:\n",
        "- How to access the relevant part of the reference genome based on the chromosome and position from the polymorphism data.\n",
        "- How to compare the reference allele in the polymorphism data with the actual nucleotide(s) at the specified position in the GRCh38 reference.\n",
        "- How to handle different types of polymorphisms (SNPs, indels) if applicable.\n",
        "- How to account for potential discrepancies in chromosome naming (e.g., 'chr1' vs '1').\n",
        "- How to use the loaded GRCh38 reference data structure (e.g., `pyfaidx` object) for efficient lookups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41ec963e"
      },
      "source": [
        "# Assuming you have loaded the GRCh38 reference using pyfaidx as suggested in the loading step:\n",
        "# from pyfaidx import Fasta\n",
        "# grch38_ref = Fasta('/path/to/GRCh38.fasta') # Replace with the actual path or object from GCS streaming/download\n",
        "\n",
        "# Assuming you have parsed the polymorphism data into a list of dictionaries:\n",
        "# polymorphisms = [\n",
        "#     {'chromosome': 'chr1', 'position': 100, 'reference_allele': 'A', 'observed_allele': 'T'},\n",
        "#     {'chromosome': '1', 'position': 150, 'reference_allele': 'G', 'observed_allele': 'C'},\n",
        "#     {'chromosome': 'chrM', 'position': 50, 'reference_allele': 'C', 'observed_allele': 'T'},\n",
        "#     # Add more polymorphism entries\n",
        "# ]\n",
        "\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def log_error(message, polymorphism=None, e=None):\n",
        "    \"\"\"Logs an error with optional polymorphism details and exception.\"\"\"\n",
        "    details = \"\"\n",
        "    if polymorphism:\n",
        "        details += f\"Polymorphism: Chromosome={polymorphism.get('chromosome')}, Position={polymorphism.get('position')}, Ref={polymorphism.get('reference_allele')}, Obs={polymorphism.get('observed_allele')}. \"\n",
        "    if e:\n",
        "        details += f\"Error: {e}\"\n",
        "    logging.error(f\"{message} {details}\")\n",
        "\n",
        "def handle_validation_error(polymorphism, error_type, e=None):\n",
        "    \"\"\"Handles and logs specific validation errors.\"\"\"\n",
        "    status_message = \"\"\n",
        "    if error_type == \"Chromosome not found\":\n",
        "        status_message = f\"Chromosome '{polymorphism.get('chromosome')}' not found in reference\"\n",
        "        log_error(status_message, polymorphism)\n",
        "    elif error_type == \"Position out of bounds\":\n",
        "        status_message = f\"Position {polymorphism.get('position')} is out of bounds for chromosome {polymorphism.get('chromosome')}\"\n",
        "        log_error(status_message, polymorphism)\n",
        "    elif error_type == \"Invalid reference allele\":\n",
        "         expected = e.args[0] if e and e.args else \"N/A\"\n",
        "         found = polymorphism.get('reference_allele')\n",
        "         status_message = f\"Invalid reference allele: Expected '{expected}', found '{found}' at {polymorphism.get('chromosome')}:{polymorphism.get('position')}\"\n",
        "         log_error(status_message, polymorphism)\n",
        "    elif error_type == \"General validation error\":\n",
        "        status_message = f\"An error occurred during validation\"\n",
        "        log_error(status_message, polymorphism, e)\n",
        "    else:\n",
        "        status_message = \"Unknown validation error\"\n",
        "        log_error(status_message, polymorphism, e)\n",
        "\n",
        "    return False, status_message\n",
        "\n",
        "\n",
        "def validate_polymorphism(polymorphism, grch38_ref):\n",
        "    \"\"\"\n",
        "    Validates a single polymorphism entry against the GRCh38 reference.\n",
        "\n",
        "    Args:\n",
        "        polymorphism (dict): A dictionary representing a single polymorphism\n",
        "                             with keys 'chromosome', 'position', 'reference_allele',\n",
        "                             and 'observed_allele'.\n",
        "        grch38_ref (pyfaidx.Fasta): The loaded GRCh38 reference genome object.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the polymorphism's reference allele matches the GRCh38 reference,\n",
        "              False otherwise.\n",
        "        str: A status message (\"Valid\", \"Invalid reference allele\", \"Chromosome not found\", \"Position out of bounds\").\n",
        "    \"\"\"\n",
        "    chrom = polymorphism.get('chromosome')\n",
        "    pos = polymorphism.get('position')\n",
        "    ref_allele_polymorphism = polymorphism.get('reference_allele')\n",
        "\n",
        "    if not chrom or pos is None or not ref_allele_polymorphism:\n",
        "        return handle_validation_error(polymorphism, \"General validation error\", e=ValueError(\"Missing required fields\"))\n",
        "\n",
        "\n",
        "    # Handle potential chromosome naming discrepancies (e.g., 'chr1' vs '1')\n",
        "    # You might need a mapping or try both 'chrX' and 'X'\n",
        "    if chrom not in grch38_ref:\n",
        "        if f'chr{chrom}' in grch38_ref:\n",
        "            chrom = f'chr{chrom}'\n",
        "        elif chrom.startswith('chr') and chrom[3:] in grch38_ref:\n",
        "             chrom = chrom[3:]\n",
        "        else:\n",
        "            return handle_validation_error(polymorphism, \"Chromosome not found\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # pyfaidx uses 1-based indexing for fetching sequences\n",
        "        # The polymorphism position is likely 1-based, so no adjustment needed for pyfaidx fetch\n",
        "        # If your polymorphism positions are 0-based, you would need to add 1: pos + 1\n",
        "        # Fetch the reference allele(s) at the specified position\n",
        "        # For a SNP, fetch a single nucleotide. For an indel, you might need to fetch a region.\n",
        "        # This example assumes SNPs for simplicity. For indels, the logic would be more complex\n",
        "        # and might involve fetching flanking sequences.\n",
        "        ref_allele_grch38 = grch38_ref[chrom][pos - 1].seq # pyfaidx uses 0-based indexing for slicing, so subtract 1\n",
        "\n",
        "        # Compare the reference alleles (case-insensitive comparison is often safer)\n",
        "        if ref_allele_polymorphism.upper() == ref_allele_grch38.upper():\n",
        "            return True, \"Valid\"\n",
        "        else:\n",
        "            return handle_validation_error(polymorphism, \"Invalid reference allele\", e=ValueError(ref_allele_grch38))\n",
        "\n",
        "    except IndexError:\n",
        "        return handle_validation_error(polymorphism, \"Position out of bounds\")\n",
        "    except Exception as e:\n",
        "        return handle_validation_error(polymorphism, \"General validation error\", e)\n",
        "\n",
        "# Example usage:\n",
        "# validated_polymorphisms = []\n",
        "# for poly in polymorphisms:\n",
        "#     is_valid, status = validate_polymorphism(poly, grch38_ref)\n",
        "#     poly['validation_status'] = status\n",
        "#     poly['is_valid'] = is_valid\n",
        "#     validated_polymorphisms.append(poly)\n",
        "\n",
        "# Now validated_polymorphisms contains the original data with added validation status.\n",
        "# You would then process this list to handle valid/invalid entries."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fc6b711"
      },
      "source": [
        "## Perform GRCh38 Checks\n",
        "\n",
        "### Subtask:\n",
        "Detail the process of comparing the polymorphism data against the loaded GRCh38 reference to validate positions and alleles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95c98508"
      },
      "source": [
        "# Task\n",
        "Implement the GRCh38 checks within the `polymorphism-processor-service` for the Google Cloud application described, ensuring the service accesses the GRCh38 reference data from the \"grch38-ref-bucket\", parses the input TXT polymorphism data, performs validation against the reference, handles the results, and integrates this logic into the service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dbe2b7d"
      },
      "source": [
        "## Load grch38 reference data\n",
        "\n",
        "### Subtask:\n",
        "Describe how the processor service will access and load the GRCh38 reference genome data from the `grch38-ref-bucket`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ed253a"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the mechanisms for accessing, locating, and loading the GRCh38 reference genome data from the specified Google Cloud Storage bucket. This involves explaining authentication, file identification, and data retrieval methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15cc284e"
      },
      "source": [
        "# 1. Authentication and Authorization\n",
        "# The polymorphism-processor-service will typically use a Service Account associated with the Google Cloud project.\n",
        "# This Service Account should have the necessary permissions (e.g., Storage Object Viewer) to access objects in the 'grch38-ref-bucket'.\n",
        "# Authentication is handled automatically when running within Google Cloud environments (like GKE, Cloud Run, or GCE) if the service is configured to use the default service account or a specifically attached service account.\n",
        "# For local development or testing, credentials can be provided through the GOOGLE_APPLICATION_CREDENTIALS environment variable pointing to a service account key file.\n",
        "\n",
        "# 2. Identifying and Locating the Reference File(s)\n",
        "# The service will need to know the name or path of the GRCh38 reference file(s) within the bucket.\n",
        "# This could be configured in the service's environment variables, a configuration file, or passed as a parameter.\n",
        "# For example, if the file is named 'GRCh38.fasta', the service would use this name to access the specific object in the bucket.\n",
        "# The service could potentially list objects in the bucket or a specific prefix if the exact file name isn't known beforehand, but knowing the file name is more efficient.\n",
        "\n",
        "# 3. Downloading or Streaming the Reference Data\n",
        "# The service will use the Google Cloud Storage client library for Python (google-cloud-storage) to interact with the bucket.\n",
        "# Data can be downloaded in chunks or streamed to avoid loading the entire potentially large file into memory at once if not necessary.\n",
        "# For validation lookup, streaming or downloading the relevant parts of the file (e.g., specific chromosomes or regions) might be more efficient than downloading the whole genome.\n",
        "# Example using the client library:\n",
        "# from google.cloud import storage\n",
        "# client = storage.Client()\n",
        "# bucket = client.get_bucket('grch38-ref-bucket')\n",
        "# blob = bucket.blob('GRCh38.fasta')\n",
        "# # To download as a string:\n",
        "# # content = blob.download_as_string()\n",
        "# # To stream:\n",
        "# # with blob.open(\"r\") as f:\n",
        "# #     for line in f:\n",
        "# #         # Process line\n",
        "\n",
        "# 4. Structuring/Storing Data in Memory for Efficient Lookup\n",
        "# The structure in memory depends on the validation process.\n",
        "# If validation requires random access to specific genomic positions, an indexed format like a FASTA index (.fai) should be used alongside the FASTA file.\n",
        "# The service can load the index into memory to quickly find the byte offset of a sequence.\n",
        "# For the sequence data itself, it might be loaded into memory partially (e.g., one chromosome at a time) or accessed by streaming based on the index lookup.\n",
        "# Libraries like `pyfaidx` in Python can be used to efficiently access sequences from indexed FASTA files without loading the entire file into memory.\n",
        "# Alternatively, if the reference data is pre-processed into a database or a more query-optimized format (like a specialized genomic database), the service could connect to that instead of directly accessing the FASTA file. For the scope of this task focusing on the GCS bucket, we assume direct access to the FASTA and its index."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ea47b0a"
      },
      "source": [
        "## Parse input polymorphism data\n",
        "\n",
        "### Subtask:\n",
        "Explain how the service will read and parse the uploaded TXT polymorphism data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f61874fb"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe how the service will read and parse the uploaded TXT polymorphism data, including how it receives the data, reads the content, parses different potential formats and expected columns, handles errors during parsing, and the in-memory data structure for the parsed data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f373cef9"
      },
      "source": [
        "# 1. Receiving the Uploaded TXT File\n",
        "# The polymorphism-processor-service will likely receive the uploaded TXT file as part of an API request.\n",
        "# This could be a file upload via an HTTP POST request if the service is exposed via a web framework (e.g., Flask, FastAPI).\n",
        "# Alternatively, if the service is triggered by an event (e.g., a file being uploaded to a different GCS bucket), the file path in GCS could be passed as a message in a Pub/Sub topic, and the service would then read the file from that GCS location using the google-cloud-storage library, similar to how the reference data is accessed.\n",
        "# For this explanation, let's assume the file content is received directly as part of an API request or read from a GCS path provided in a message.\n",
        "\n",
        "# 2. Reading the Content of the TXT File\n",
        "# If the file content is received in an API request, it will be available in the request body or as a file object provided by the web framework.\n",
        "# If reading from GCS, the google-cloud-storage client library will be used to download the file content as a string or stream it.\n",
        "# The content will be read as plain text. It's important to handle potential character encodings (e.g., UTF-8).\n",
        "\n",
        "# 3. Parsing the Read Data\n",
        "# The service will need to parse the text content line by line.\n",
        "# Each line is expected to represent a single polymorphism entry.\n",
        "# The format of the line needs to be known or inferred. Common formats include tab-separated values (TSV) or comma-separated values (CSV).\n",
        "# The service will split each line based on the delimiter (tab or comma).\n",
        "# The expected columns and their order must be defined. A typical format might include:\n",
        "# - Chromosome (e.g., 'chr1', '1')\n",
        "# - Position (an integer)\n",
        "# - Reference Allele (a nucleotide or sequence)\n",
        "# - Observed Allele (a nucleotide or sequence)\n",
        "# The service will need to convert the data in each column to the appropriate data type (e.g., position to integer).\n",
        "\n",
        "# Example parsing logic (assuming tab-separated):\n",
        "# import io\n",
        "# def parse_polymorphism_data(text_content):\n",
        "#     polymorphisms = []\n",
        "#     # Use io.StringIO to treat the string content as a file\n",
        "#     data_file = io.StringIO(text_content)\n",
        "#     for line in data_file:\n",
        "#         line = line.strip()\n",
        "#         if not line or line.startswith('#'): # Skip empty lines or comments\n",
        "#             continue\n",
        "#         fields = line.split('\\t') # Assuming tab-separated\n",
        "#         if len(fields) >= 4: # Ensure minimum expected columns are present\n",
        "#             try:\n",
        "#                 chromosome = fields[0]\n",
        "#                 position = int(fields[1])\n",
        "#                 reference_allele = fields[2]\n",
        "#                 observed_allele = fields[3]\n",
        "#                 polymorphisms.append({\n",
        "#                     'chromosome': chromosome,\n",
        "#                     'position': position,\n",
        "#                     'reference_allele': reference_allele,\n",
        "#                     'observed_allele': observed_allele\n",
        "#                 })\n",
        "#             except ValueError as e:\n",
        "#                 # Handle error: position is not an integer\n",
        "#                 print(f\"Error parsing line: {line} - {e}\")\n",
        "#                 # Depending on requirements, log the error, skip the line, or return an error\n",
        "#             except IndexError as e:\n",
        "#                  # Handle error: not enough fields\n",
        "#                 print(f\"Error parsing line (missing fields): {line} - {e}\")\n",
        "#         else:\n",
        "#             # Handle error: incorrect number of fields\n",
        "#             print(f\"Skipping malformed line: {line}\")\n",
        "#     return polymorphisms\n",
        "\n",
        "\n",
        "# 4. Handling Potential Errors During Parsing\n",
        "# Errors can occur due to:\n",
        "# - Incorrect delimiter (e.g., comma instead of tab)\n",
        "# - Incorrect number of columns in a line\n",
        "# - Data in a column not being of the expected type (e.g., non-integer position)\n",
        "# - Invalid characters or encoding issues\n",
        "# The service should implement error handling for these cases.\n",
        "# - For minor errors (e.g., a few malformed lines), the service might log the error and skip the problematic line, continuing with the valid data.\n",
        "# - For major errors (e.g., the entire file has the wrong format), the service might reject the file and return an error response to the user or triggering system.\n",
        "# - Specific exceptions (like ValueError, IndexError) should be caught during parsing.\n",
        "# - Providing informative error messages about the line number and type of error will be helpful for debugging.\n",
        "\n",
        "# 5. In-Memory Data Structure for Parsed Data\n",
        "# The parsed polymorphism data can be stored in a list of dictionaries or objects.\n",
        "# A list of dictionaries is a simple and flexible structure in Python. Each dictionary would represent a polymorphism entry with keys for 'chromosome', 'position', 'reference_allele', and 'observed_allele'.\n",
        "# Example:\n",
        "# [\n",
        "#     {'chromosome': 'chr1', 'position': 100, 'reference_allele': 'A', 'observed_allele': 'T'},\n",
        "#     {'chromosome': 'chr1', 'position': 150, 'reference_allele': 'G', 'observed_allele': 'C'},\n",
        "#     ...\n",
        "# ]\n",
        "# For very large files, consider using a more memory-efficient structure or processing data in chunks if feasible for the validation logic. However, a list of dictionaries is a good starting point for moderate file sizes and ease of use. Pandas DataFrame could also be used for more complex data manipulation and cleaning, but might be overkill for simple validation lookup."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feb602dc"
      },
      "source": [
        "# Task\n",
        "Set up a Google Cloud project and services for a data processing pipeline with a web interface for user interaction, containerize the processor service and frontend application, build and push container images, deploy Cloud Run services, set up Cloud Storage buckets, configure a Global Load Balancer, set up a GitHub repository, integrate with GitHub for CI/CD, sync code with GitHub, and finally test and monitor the application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66e871a5"
      },
      "source": [
        "## Set up google cloud project and services\n",
        "\n",
        "### Subtask:\n",
        "Configure your Google Cloud project, including enabling necessary APIs (Cloud Run, Cloud Storage, Pub/Sub), and setting up service accounts with appropriate permissions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4347d2f"
      },
      "source": [
        "## Containerize the processor service\n",
        "\n",
        "### Subtask:\n",
        "Create a Dockerfile for the `polymorphism-processor-service` to package it as a container image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecd19fed"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a Dockerfile for the polymorphism-processor-service with the specified steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3f3b1fc",
        "outputId": "4acd2fe4-91de-4a1a-e217-12863337685d"
      },
      "source": [
        "%%writefile Dockerfile\n",
        "# Use an official Python runtime as a parent image\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the current directory contents into the container at /app\n",
        "COPY . /app\n",
        "\n",
        "# Install any needed packages specified in requirements.txt\n",
        "# Assume a requirements.txt file exists with necessary dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Make port 8080 available to the world outside this container\n",
        "# Assuming the service runs on port 8080, adjust if necessary\n",
        "EXPOSE 8080\n",
        "\n",
        "# Define environment variable\n",
        "ENV NAME World\n",
        "\n",
        "# Run app.py when the container launches\n",
        "# Adjust 'main.py' to your service's entry point file name\n",
        "CMD [\"python\", \"main.py\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46e54123"
      },
      "source": [
        "## Containerize the frontend application\n",
        "\n",
        "### Subtask:\n",
        "Create Dockerfiles for the `webapp-frontend` (HTML, CSS, JS files served by a web server like Nginx or a simple Python server).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e61038cc"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the Dockerfile for the frontend application using Nginx as the base image and copy the necessary files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea71e523",
        "outputId": "1c478cf7-02ae-4c40-fb77-51685ca028ec"
      },
      "source": [
        "%%writefile Dockerfile.frontend\n",
        "# Use an official Nginx image as a base\n",
        "FROM nginx:latest\n",
        "\n",
        "# Copy the frontend application files into the default Nginx html directory\n",
        "# Assume your frontend files are in a directory named 'frontend' relative to the Dockerfile\n",
        "COPY ./frontend /usr/share/nginx/html\n",
        "\n",
        "# Expose port 80, the default port for Nginx\n",
        "EXPOSE 80\n",
        "\n",
        "# The default command of the Nginx image starts the server, so no explicit CMD is needed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile.frontend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cb7f4ae"
      },
      "source": [
        "## Build and push container images\n",
        "\n",
        "### Subtask:\n",
        "Build the Docker images for both services and push them to Google Cloud Container Registry or Artifact Registry.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "049b8922"
      },
      "source": [
        "**Reasoning**:\n",
        "Authenticate Docker with Google Cloud and then build and tag the Docker images for both the processor service and the frontend application based on the instructions provided.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f342742",
        "outputId": "e7c42f54-8447-4be3-f363-1a964fca443f"
      },
      "source": [
        "!gcloud auth configure-docker us-central1-docker.pkg.dev\n",
        "\n",
        "# Replace 'your-gcp-project-id' and 'your-repo' with your actual GCP project ID and Artifact Registry repository name\n",
        "PROJECT_ID = 'your-gcp-project-id'\n",
        "REPO_NAME = 'your-repo'\n",
        "PROCESSOR_IMAGE = f'us-central1-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/polymorphism-processor-service:latest'\n",
        "FRONTEND_IMAGE = f'us-central1-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/webapp-frontend:latest'\n",
        "\n",
        "# Build and tag the processor service image\n",
        "!docker build -t {PROCESSOR_IMAGE} -f Dockerfile .\n",
        "\n",
        "# Build and tag the frontend application image\n",
        "# Assuming your frontend files are in a directory named 'frontend' relative to the Dockerfile.frontend\n",
        "# If your frontend files are in the same directory as Dockerfile.frontend, remove the './frontend' part in the COPY instruction in Dockerfile.frontend\n",
        "!docker build -t {FRONTEND_IMAGE} -f Dockerfile.frontend .\n",
        "\n",
        "# Push both images to Artifact Registry\n",
        "!docker push {PROCESSOR_IMAGE}\n",
        "!docker push {FRONTEND_IMAGE}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;33mWARNING:\u001b[0m `docker` not in system PATH.\n",
            "`docker` and `docker-credential-gcloud` need to be in the same PATH in order to work correctly together.\n",
            "gcloud's Docker credential helper can be configured but it will not work until this is corrected.\n",
            "Adding credentials for: us-central1-docker.pkg.dev\n",
            "After update, the following will be written to your Docker config file located \n",
            "at [/root/.docker/config.json]:\n",
            " {\n",
            "  \"credHelpers\": {\n",
            "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
            "  }\n",
            "}\n",
            "\n",
            "Do you want to continue (Y/n)?  \n",
            "\n",
            "Command killed by keyboard interrupt\n",
            "\n",
            "\n",
            "\u001b[1;31mERROR:\u001b[0m gcloud crashed (RuntimeError): reentrant call inside <_io.BufferedWriter name='<stderr>'>\n",
            "\n",
            "If you would like to report this issue, please run the following command:\n",
            "  gcloud feedback\n",
            "\n",
            "To check gcloud for common problems, please run the following command:\n",
            "  gcloud info --run-diagnostics\n",
            "/bin/bash: line 1: docker: command not found\n",
            "/bin/bash: line 1: docker: command not found\n",
            "/bin/bash: line 1: docker: command not found\n",
            "/bin/bash: line 1: docker: command not found\n"
          ]
        }
      ]
    }
  ]
}